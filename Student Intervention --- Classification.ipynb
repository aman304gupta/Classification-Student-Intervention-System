{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A system which interferes if a student is about to fail. Classifcation because output 1 (intervention is required)\n",
    "## 0 (intervention is not required) Output is discrete not continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score ## It is harmonic mean of both precision and recall\n",
    "\n",
    "student_data = pd.read_csv(\"student-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data.head()  ## prints 1st 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_data.shape  ## not.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_students = student_data.shape[0]  ## .shape IMp \n",
    "n_features = student_data.shape[1] - 1 ## -1 imp to remove output label\n",
    "\n",
    "passed = student_data.loc[student_data.passed == \"yes\",\"passed\"] ## .loc searches with labels while iloc searches with location\n",
    "n_passed = passed.shape[0]\n",
    "\n",
    "failed = student_data.loc[student_data.passed == \"no\",\"passed\"]\n",
    "n_failed = failed.shape[0]\n",
    "\n",
    "total = float(n_passed + n_failed) ## Typecating to float\n",
    "grad_rate = float((n_passed *100)/total)\n",
    "\n",
    "print(\"Data Exploration--->\")\n",
    "print(\"Total Students\\t{}\".format(n_students))\n",
    "print(\"Total Passed\\t{}\".format(n_passed))\n",
    "print(\"Total Failed\\t{}\".format(n_failed))\n",
    "print(\"Graduation rate\\t{:.2f}%\".format(grad_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(student_data.columns[:-1]) ##.columns yileds column names in array format list() converts them to list fromat\n",
    "target_col = student_data.columns[-1] ## Acces last index in columns array i.e y\n",
    "\n",
    "print(\"Features Colums\\n{}\".format(feature_cols))\n",
    "print(\"Target Column\\n{}\".format(target_col))\n",
    "\n",
    "X_all = student_data[feature_cols]\n",
    "Y_all = student_data[target_col]\n",
    "\n",
    "X_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing--->Convert yes/no to 1/0\n",
    "## variables having more than to categories are fed to pd.get_dummies()\n",
    "def preprocess_features(X):\n",
    "    \n",
    "    output = pd.DataFrame(index=X.index) ## Creates an empty dataframe with index same as data\n",
    "    \n",
    "    for col, col_data in X.iteritems(): ## Iterating over columns---\n",
    "        \n",
    "        if col_data.dtype == object:  ## if data has dtype of non-numerical\n",
    "            col_data = col_data.replace(['yes','no'],[1,0])  ## replacing yes/no to 1/0\n",
    "            \n",
    "        if col_data.dtype == object:  ## if data has dtype of categorical\n",
    "            ### Example school is converted to school_gp, school_ms\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  \n",
    "        \n",
    "        output = output.join(col_data)\n",
    "        \n",
    "    return output\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print(\"Processed feature columns ({} total columns):\\t{}\".format(len(X_all.columns),list(X_all.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_all, Y_all, stratify = Y_all, test_size=95, random_state=42)\n",
    "\n",
    "# Show the results of the split\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since this is classification we are going to use\n",
    "## 1. Naive Bayes--> It performs well on small dataset\n",
    "## 2. logistic Regression --> Since there is a lot of correlation among features, also we could regularization ter,\n",
    "## 3. SVM\n",
    "## Then choice best of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf,X_train,Y_train): ## Function to train classifier\n",
    "    \n",
    "    start = time()\n",
    "    clf.fit(X_train,Y_train)\n",
    "    end = time()\n",
    "    \n",
    "    print(\" Model trained in {:.4f} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(clf,features,target): ## Model to predict labels can be for training/testing both\n",
    "    \n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    print(\"Model predicted value in {:.4f}seconds\".format(end-start))\n",
    "    \n",
    "    return f1_score(target.values,y_pred,pos_label=\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {:.4f}.\\n\".format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clf_A = GaussianNB()\n",
    "clf_B = LogisticRegression(random_state=42)\n",
    "clf_C = SVC(random_state=42)\n",
    "\n",
    "X_train_100 = X_train.iloc[:100, :] ## i.e 1st using top 100 data\n",
    "Y_train_100 = Y_train.iloc[:100]\n",
    "\n",
    "X_train_200 = X_train.iloc[:200, :]\n",
    "Y_train_200 = Y_train.iloc[:200]\n",
    "\n",
    "X_train_300 = X_train.iloc[:300, :]\n",
    "Y_train_300 = Y_train.iloc[:300]\n",
    "\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    print (\"\\n{}: \\n\\n\".format(clf.__class__.__name__))\n",
    "    for n in [100, 200, 300]:\n",
    "        train_predict(clf, X_train[:n], Y_train[:n], X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can clearly see the results are much better in SVM\n",
    "### BUT SvM has O(n^3) hence once the data increses the computation time would increse tremendously \n",
    "### Hence we will choice logistic regression\n",
    "\n",
    "## GridSearchCV ---> In this method we exhaustively search over a given a set of values for a given hyperparametr \n",
    "## finding the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100, 1000] ## Different values of C in LIST!!!\n",
    "solver = ['sag']\n",
    "max_iter = [1000]\n",
    "param_grid = dict(C=C, solver=solver, max_iter=max_iter)\n",
    "\n",
    "clf = LogisticRegression(random_state=42) ## Our classifer \n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label='yes') ## Thsi is the scoring function\n",
    "##We will use a stratified shuffle split data-split since it preserves the percentage of samples \n",
    "## for each class and combines it with cross validation\n",
    "ssscv = StratifiedShuffleSplit(Y_train, n_iter=10, test_size=0.1)  \n",
    "\n",
    "## CV parameter is our cross validation generator\n",
    "grid_obj = GridSearchCV(clf, param_grid, cv=ssscv, scoring=f1_scorer) ### Scoring--> how we have to measure\n",
    "\n",
    "grid_obj = grid_obj.fit(X_train, Y_train) ## Fitting the data\n",
    "\n",
    "clf = grid_obj.best_estimator_ ## FInding the best parameters\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, Y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
